**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group: 10    |
|-----------------|
| Mohamed Ebdalla                |   
| Aryan Karadia              |   
| Raisa Rafi               |   
| Zoraiz Khan             |   

# Introduction
Software reliability assessment plays an important role in ensuring the quality of a software system. Using reliability assessment tools, we are able to gain valuable information of the performance of a system under various conditions. There are many available techniques to help assess system reliability, but in this lab, we are going to focus on two. Specifically, we will be analyzing reliability growth testing and reliability demonstration charts (RDC).  

Part 1: Reliability Growth Testing  
In this section, we explored the concept of reliability growth testing by using the provided failure data to evaluate the reliability of the system. After processing the input values, we used C-SFRAT to measure the failure rate and the Mean Time To Failure (MTTF). Furthermore, we analyzed the graphs produced and drew conclusions on the system reliability based on what we observed.

Part 2: Reliability Demonstration Chart (RDC)  
In this part of the assignment, we used a reliability demonstration chart to assess the reliability of the system. We used the provided failure data to generate an RDC on excel showing whether or not the target failure rate or MTTF is met. This was done by plotting data at different points in time and analyzing the trends, observing what happens when we halve and double the MTTFmin value.

# Assessment Using Reliability Growth Testing 

## Selected Models for Best Fit

Our model ranking is as follows:

1. Truncated Logistic
2. Discrete Weibull (Type III)

These two models had the lowest Akaike information criterion (AIC) and Bayesian Information Criteria (BIC) values when looking at the model comparisons below:
![alt text](media/model_comparison.png)

## Selecting Range of Useful Data

From our testing we found that using 70% of the data gets the best results. Using the range of data points 1-55 (70%) of the data we get the lowest AIC and BIC values without grossly overfitting to the data.

## Displaying Graphs

Here you can see both graphs displayed together onto the data.

![alt text](media/image.png)

### TL Graph
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/31897a5b-7773-4e3a-9591-a63bdb472f6d)

### DW3 Graph
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/ac73f268-0398-428f-8d86-0314f6b76d5d)

### Time Between Failures
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/d5532c4e-d6be-4ca7-ae05-bdaada334ebf)

Since the data being used is a Time, Failure Count table, the time between failure is usually constant and only changes if FC is 0.

### Failure Intensity
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/06cd3256-bbd7-4a2d-9681-0ccac36f7233)


### Reliability
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/61c5532e-43ad-4a8a-93a3-159f511b3f13)


## Discussing Acceptable Range of Failure Rate
To determine the acceptable range of failure rate, we decided to use the Interquartile Range Method to determe outliers. The steps below outlined our process:

1. Sort the data in ascending order
2. Calculate quartiles (Q1,Q2,Q3)
3. Calculate IQR
4. Identify and remove all outliers

After sorting the data, the quartiles were identified to be:

Q1 = 0, Q2 = 1, Q3 = 2

IQR was determined to as:

IQR = Q3-Q1 = 2-0 = 2

To identify outliers:

Upper Limit = Q3+1.5(IQR) = 2 + 1.5(2) = 5

Lower Limit = Q1-1.5(IQR) = 2 + 1.5(2) = -3

Because time in intervals cannot be negative, our outliers were only values of failure count which were above 5. Thus, our acceptable range of failure rate was determined to be between 0-5 inclusively.

Plotting this data (file "DataSet3_Removed_Outliers.csv") using our model ranking:

### DW3 Graph Within Acceptable Range
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/95386597/659c1804-341a-44cc-b0be-a6bcb8c27135)


# Assessment Using Reliability Demonstration Chart 

## MTTFmin = 0.0038
![image](media/MTTFmin.png)

## MTTFmin half = 0.0019
![image](media/MTTFmin_half.png)

## MTTFmin twice = 0.0076
![image](media/MTTFmin_double.png)

# Explain your evaluation and justification of how you decide the MTTFmin

We used the "Data6.dat" file as input for failure data and we generated the reliability demonstration chart using the RDC Excel tool. We then experimented with various values for FIO, which helped us pinpoint the one where the plotted line falls just within the acceptable region of the chart. This yielded an approximate MTTFmin of 0.0038.

In the first graph, where the MTTFmin=0.0038, the line starts within the 'continue' region but gradually transitions into the acceptable region. Conversely, in the next graph where MTTFmin is halved, the line quickly enters the 'accept' region, signifying the reliablity of the SUT. And finally, upon doubling MTTFmin, the line moves into the 'reject' region, deeming it unacceptale and below the required standard.


# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques  
Some similarities between reliability growth testing and reliability demonstration charts is that they're both techniques used to assess the reliability of a system, providing information about the system's performance in terms of failure rates and mean time to failure (MTTF). Another thing in common between the two techniques is that they both use failure data collected during testing and then that data is used to analyze trends and patterns. Lastly, both techniques involve statistical analysis to interpret failure data including fitting models, calculating metrics like AIC and BIC, and determining confidence intervals.  

While both techniques are similar in what they do, they have different approaches and serve different purposes. For one, reliability growth testing focuses on the evolution of reliability over time as testing progresses while reliability demonstration charts (RDC) focus on showing whether the system meets predefined reliability targets. Reliability growth testing fits reliability growth models to failure data while RDCs plots failure data against reliability targets to determine whether or not the system meets these targets. Lastly, reliability growth testing helps to make decisions regarding further testing, identifying areas for improvement, and predicting future reliability performance while RDCs helps make decisions regarding the acceptability of the current system.

# How the team work/effort was divided and managed  
Two members attempted part 1 and other other two attempted part 2. Everybody contributed to the lab report based on the parts of the lab that they completed.

# Difficulties encountered, challenges overcome, and lessons learned  
One of the challenges we faced was formatting the input data prior to working with it. We were confused about how to format and use the data, but referred to resources online to help us. Another difficulty we encountered was trying to get SRTAT to work, so we ended up using C-SFRAT.

# Comments/feedback on the lab itself  
The lab instructions were easy to follow and were quite informative. It helped us understand how to use both reliability growth testing and reliability demonstration charts to assess system reliability, as well as the similarities/differences between the two techniques.
