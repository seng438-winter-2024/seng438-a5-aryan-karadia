**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group: 10    |
|-----------------|
| Mohamed Ebdalla                |   
| Aryan Karadia              |   
| Raisa Rafi               |   
| Zoraiz Khan             |   

# Introduction
Software reliability assessment plays an important role in ensuring the quality of a software system. Using reliability assessment tools, we are able to gain valuable information of the performance of a system under various conditions. There are many available techniques to help assess system reliability, but in this lab, we are going to focus on two. Specifically, we will be analyzing reliability growth testing and reliability demonstration charts (RDC).  

Part 1: Reliability Growth Testing  
In this section, we explored the concept of reliability growth testing by using the provided failure data to evaluate the reliability of the system. After processing the input values, we used C-SFRAT to measure the failure rate and the Mean Time To Failure (MTTF). Furthermore, we analyzed the graphs produced and drew conclusions on the system reliability based on what we observed.

Part 2: Reliability Demonstration Chart (RDC)  
In this part of the assignment, we used a reliability demonstration chart to assess the reliability of the system. We used the provided failure data to generate an RDC on excel showing whether or not the target failure rate or MTTF is met. This was done by plotting data at different points in time and analyzing the trends, observing what happens when we halve and double the MTTFmin value.

# Assessment Using Reliability Growth Testing 

## Selected Models for Best Fit

Our model ranking is as follows:

1. Truncated Logistic
2. Discrete Weibull (Type III)

These two models had the lowest Akaike information criterion (AIC) and Bayesian Information Criteria (BIC) values when looking at the model comparisons below:
![alt text](media/model_comparison.png)

## Selecting Range of Useful Data

From our testing we found that using 70% of the data gets the best results. Using the range of data points 1-55 (70%) of the data we get the lowest AIC and BIC values without grossly overfitting to the data.

## Displaying Graphs

Here you can see both graphs displayed together onto the data.

![alt text](media/image.png)

### TL Graph
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/31897a5b-7773-4e3a-9591-a63bdb472f6d)

### DW3 Graph
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/ac73f268-0398-428f-8d86-0314f6b76d5d)

### Time Between Failures
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/d5532c4e-d6be-4ca7-ae05-bdaada334ebf)

Since the data being used is a Time, Failure Count table, the time between failure is usually constant and only changes if FC is 0.

### Failure Intensity
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/06cd3256-bbd7-4a2d-9681-0ccac36f7233)


### Reliability
![image](https://github.com/seng438-winter-2024/seng438-a5-aryan-karadia/assets/105018373/61c5532e-43ad-4a8a-93a3-159f511b3f13)


## Discussing Acceptable Range of Failure Rate
To determine the acceptable range of failure rate, we decided to use the Interquartile Range Method to determe outliers. The steps below outlined our process:

1. Sort the data in ascending order
2. Calculate quartiles (Q1,Q2,Q3)
3. Calculate IQR
4. Identify and remove all outliers

After sorting the data, the quartiles were identified to be:

Q1 = 3
Q3 = 9
IQR = Q3 - Q1 = 9 - 3 = 6
Now, we can identify the upper and lower bounds to detect outliers:

Lower Bound = Q1 - 1.5 * IQR = 3 - 1.5 * 6 = 3 - 9 = -6
Upper Bound = Q3 + 1.5 * IQR = 9 + 1.5 * 6 = 9 + 9 = 18

Any data point below the lower bound or above the upper bound is considered an outlier.

Because time in intervals cannot be negative, our outliers were only values of failure count which were above 18. Thus, our acceptable range of failure rate was determined to be between 0-18 inclusively.

Then, the new data is stored in a file called "DataSet3_Removed_Outliers.csv"

Further, we can calculate the Mean failure rate and standard deviation of failure rate before the outliers:
**Mean failure rate**: 5.395061728395062

**Standard deviation of failure rate**: 5.315779009484036

**Acceptable range of failure rate**: -5.23649629057301 to 16.026619747363135

And the same metrics after removing the outliers:

**Mean failure rate after removing outliers**: 5.1875

**Standard deviation of failure rate after removing outliers**: 4.856294240467725

**Acceptable range of failure rate after removing outliers**: -4.52508848093545 to 14.90008848093545


# Assessment Using Reliability Demonstration Chart 

## MTTFmin = 0.0038
![image](media/MTTFmin.png)

## MTTFmin half = 0.0019
![image](media/MTTFmin_half.png)

## MTTFmin twice = 0.0076
![image](media/MTTFmin_double.png)

# Explain your evaluation and justification of how you decide the MTTFmin

We used the "Data6.dat" file as input for failure data and we generated the reliability demonstration chart using the RDC Excel tool. We then experimented with various values for FIO, which helped us pinpoint the one where the plotted line falls just within the acceptable region of the chart. This yielded an approximate MTTFmin of 0.0038.

In the first graph, where the MTTFmin=0.0038, the line starts within the 'continue' region but gradually transitions into the acceptable region. Conversely, in the next graph where MTTFmin is halved, the line quickly enters the 'accept' region, signifying the reliablity of the SUT. And finally, upon doubling MTTFmin, the line moves into the 'reject' region, deeming it unacceptale and below the required standard.


# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques  
Some similarities between reliability growth testing and reliability demonstration charts is that they're both techniques used to assess the reliability of a system, providing information about the system's performance in terms of failure rates and mean time to failure (MTTF). Another thing in common between the two techniques is that they both use failure data collected during testing and then that data is used to analyze trends and patterns. Lastly, both techniques involve statistical analysis to interpret failure data including fitting models, calculating metrics like AIC and BIC, and determining confidence intervals.  

While both techniques are similar in what they do, they have different approaches and serve different purposes. For one, reliability growth testing focuses on the evolution of reliability over time as testing progresses while reliability demonstration charts (RDC) focus on showing whether the system meets predefined reliability targets. Reliability growth testing fits reliability growth models to failure data while RDCs plots failure data against reliability targets to determine whether or not the system meets these targets. Lastly, reliability growth testing helps to make decisions regarding further testing, identifying areas for improvement, and predicting future reliability performance while RDCs helps make decisions regarding the acceptability of the current system.

# How the team work/effort was divided and managed  
Two members attempted part 1 and other other two attempted part 2. Everybody contributed to the lab report based on the parts of the lab that they completed.

# Difficulties encountered, challenges overcome, and lessons learned  
One of the challenges we faced was formatting the input data prior to working with it. We were confused about how to format and use the data, but referred to resources online to help us. Another difficulty we encountered was trying to get SRTAT to work, so we ended up using C-SFRAT.

# Comments/feedback on the lab itself  
The lab instructions were easy to follow and were quite informative. It helped us understand how to use both reliability growth testing and reliability demonstration charts to assess system reliability, as well as the similarities/differences between the two techniques.
